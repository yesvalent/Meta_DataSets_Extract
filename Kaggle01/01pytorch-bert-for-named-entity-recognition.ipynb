{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011948,
     "end_time": "2021-03-31T14:13:55.209051",
     "exception": false,
     "start_time": "2021-03-31T14:13:55.197103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n",
    "\n",
    "Note that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010269,
     "end_time": "2021-03-31T14:13:55.229893",
     "exception": false,
     "start_time": "2021-03-31T14:13:55.219624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-31T14:13:55.261501Z",
     "iopub.status.busy": "2021-03-31T14:13:55.260850Z",
     "iopub.status.idle": "2021-03-31T14:14:24.802861Z",
     "shell.execute_reply": "2021-03-31T14:14:24.802212Z"
    },
    "papermill": {
     "duration": 29.562517,
     "end_time": "2021-03-31T14:14:24.803051",
     "exception": false,
     "start_time": "2021-03-31T14:13:55.240534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.3.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.2)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2020.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.55.1\r\n",
      "    Uninstalling tqdm-4.55.1:\r\n",
      "      Successfully uninstalled tqdm-4.55.1\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.0)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.9.4\r\n",
      "    Uninstalling tokenizers-0.9.4:\r\n",
      "      Successfully uninstalled tokenizers-0.9.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "transformers 4.2.2 requires tokenizers==0.9.4, but you have tokenizers 0.10.1 which is incompatible.\u001b[0m\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2020.11.13)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.8)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.43)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.2)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.0)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.2.2\r\n",
      "    Uninstalling transformers-4.2.2:\r\n",
      "      Successfully uninstalled transformers-4.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.0.1 requires transformers<4.3,>=4.1, but you have transformers 4.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017014,
     "end_time": "2021-03-31T14:14:24.838019",
     "exception": false,
     "start_time": "2021-03-31T14:14:24.821005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:24.878808Z",
     "iopub.status.busy": "2021-03-31T14:14:24.876412Z",
     "iopub.status.idle": "2021-03-31T14:14:25.509077Z",
     "shell.execute_reply": "2021-03-31T14:14:25.508422Z"
    },
    "papermill": {
     "duration": 0.654299,
     "end_time": "2021-03-31T14:14:25.509200",
     "exception": false,
     "start_time": "2021-03-31T14:14:24.854901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:25.547532Z",
     "iopub.status.busy": "2021-03-31T14:14:25.546828Z",
     "iopub.status.idle": "2021-03-31T14:14:26.186500Z",
     "shell.execute_reply": "2021-03-31T14:14:26.186036Z"
    },
    "papermill": {
     "duration": 0.660312,
     "end_time": "2021-03-31T14:14:26.186650",
     "exception": false,
     "start_time": "2021-03-31T14:14:25.526338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016732,
     "end_time": "2021-03-31T14:14:26.220616",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.203884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:26.259198Z",
     "iopub.status.busy": "2021-03-31T14:14:26.258566Z",
     "iopub.status.idle": "2021-03-31T14:14:26.261422Z",
     "shell.execute_reply": "2021-03-31T14:14:26.261041Z"
    },
    "papermill": {
     "duration": 0.024086,
     "end_time": "2021-03-31T14:14:26.261583",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.237497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64 # max no. words for each sentence.\n",
    "OVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "\n",
    "MAX_SAMPLE = None # set a small number for experimentation, set None for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016579,
     "end_time": "2021-03-31T14:14:26.295066",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.278487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:26.335461Z",
     "iopub.status.busy": "2021-03-31T14:14:26.334961Z",
     "iopub.status.idle": "2021-03-31T14:14:26.445994Z",
     "shell.execute_reply": "2021-03-31T14:14:26.446917Z"
    },
    "papermill": {
     "duration": 0.135135,
     "end_time": "2021-03-31T14:14:26.447111",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.311976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. raw training rows: 19661\n"
     ]
    }
   ],
   "source": [
    "train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\n",
    "paper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = train[:MAX_SAMPLE]\n",
    "print(f'No. raw training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017081,
     "end_time": "2021-03-31T14:14:26.482430",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.465349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Group by publication, training labels should have the same form as expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:26.530956Z",
     "iopub.status.busy": "2021-03-31T14:14:26.530137Z",
     "iopub.status.idle": "2021-03-31T14:14:26.885330Z",
     "shell.execute_reply": "2021-03-31T14:14:26.884340Z"
    },
    "papermill": {
     "duration": 0.385802,
     "end_time": "2021-03-31T14:14:26.885467",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.499665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. grouped training rows: 14316\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:14:26.926902Z",
     "iopub.status.busy": "2021-03-31T14:14:26.926153Z",
     "iopub.status.idle": "2021-03-31T14:15:27.279765Z",
     "shell.execute_reply": "2021-03-31T14:15:27.278847Z"
    },
    "papermill": {
     "duration": 60.376813,
     "end_time": "2021-03-31T14:15:27.279918",
     "exception": false,
     "start_time": "2021-03-31T14:14:26.903105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "papers = {}\n",
    "for paper_id in train['Id'].unique():\n",
    "    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018038,
     "end_time": "2021-03-31T14:15:27.316179",
     "exception": false,
     "start_time": "2021-03-31T14:15:27.298141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transform data to NER format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:15:27.365715Z",
     "iopub.status.busy": "2021-03-31T14:15:27.364296Z",
     "iopub.status.idle": "2021-03-31T14:15:27.366859Z",
     "shell.execute_reply": "2021-03-31T14:15:27.367413Z"
    },
    "papermill": {
     "duration": 0.03311,
     "end_time": "2021-03-31T14:15:27.367571",
     "exception": false,
     "start_time": "2021-03-31T14:15:27.334461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_training_text(txt):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions\n",
    "\n",
    "def tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = 'B'\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = 'I'\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:15:27.411416Z",
     "iopub.status.busy": "2021-03-31T14:15:27.410941Z",
     "iopub.status.idle": "2021-03-31T14:18:28.845379Z",
     "shell.execute_reply": "2021-03-31T14:18:28.844920Z"
    },
    "papermill": {
     "duration": 181.459662,
     "end_time": "2021-03-31T14:18:28.845532",
     "exception": false,
     "start_time": "2021-03-31T14:15:27.385870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 47202 positives + 514263 negatives: 100%|██████████| 14316/14316 [03:00<00:00, 91.95it/s]"
     ]
    }
   ],
   "source": [
    "cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n",
    "ner_data = []\n",
    "\n",
    "pbar = tqdm(total=len(train))\n",
    "for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n",
    "    # paper\n",
    "    paper = papers[id]\n",
    "    \n",
    "    # labels\n",
    "    labels = dataset_label.split('|')\n",
    "    labels = [clean_training_text(label) for label in labels]\n",
    "    \n",
    "    # sentences\n",
    "    sentences = set([clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.') \n",
    "                ])\n",
    "    sentences = shorten_sentences(sentences) # make sentences short\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "    \n",
    "    # positive sample\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = tag_sentence(sentence, labels)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            ner_data.append(tags)\n",
    "            cnt_neg += 1\n",
    "    \n",
    "    # process bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "\n",
    "# shuffling\n",
    "random.shuffle(ner_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 4.721935,
     "end_time": "2021-03-31T14:18:37.821344",
     "exception": false,
     "start_time": "2021-03-31T14:18:33.099409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "write data to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:18:46.558503Z",
     "iopub.status.busy": "2021-03-31T14:18:46.508183Z",
     "iopub.status.idle": "2021-03-31T14:19:18.969761Z",
     "shell.execute_reply": "2021-03-31T14:19:18.968730Z"
    },
    "papermill": {
     "duration": 36.80101,
     "end_time": "2021-03-31T14:19:18.969922",
     "exception": false,
     "start_time": "2021-03-31T14:18:42.168912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 47202 positives + 514263 negatives: 100%|██████████| 14316/14316 [03:20<00:00, 91.95it/s]"
     ]
    }
   ],
   "source": [
    "with open('train_ner.json', 'w') as f:\n",
    "    for row in ner_data:\n",
    "        words, nes = list(zip(*row))\n",
    "        row_json = {'tokens' : words, 'tags' : nes}\n",
    "        json.dump(row_json, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 4.295647,
     "end_time": "2021-03-31T14:19:27.988053",
     "exception": false,
     "start_time": "2021-03-31T14:19:23.692406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tune a BERT model for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T14:19:37.625595Z",
     "iopub.status.busy": "2021-03-31T14:19:37.590231Z",
     "iopub.status.idle": "2021-03-31T18:28:51.185824Z",
     "shell.execute_reply": "2021-03-31T18:28:51.186872Z"
    },
    "papermill": {
     "duration": 14957.854814,
     "end_time": "2021-03-31T18:28:51.187145",
     "exception": false,
     "start_time": "2021-03-31T14:19:33.332331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-31 14:19:42.156628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-84b3c2d9940dba54/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-84b3c2d9940dba54/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|file_utils.py:1402] 2021-03-31 14:20:14,577 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzcaqfpns\r\n",
      "Downloading: 100%|██████████████████████████████| 433/433 [00:00<00:00, 339kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-03-31 14:20:14,632 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\r\n",
      "[INFO|file_utils.py:1409] 2021-03-31 14:20:14,632 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\r\n",
      "[INFO|configuration_utils.py:472] 2021-03-31 14:20:14,633 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\r\n",
      "[INFO|configuration_utils.py:508] 2021-03-31 14:20:14,634 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:472] 2021-03-31 14:20:14,692 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\r\n",
      "[INFO|configuration_utils.py:508] 2021-03-31 14:20:14,693 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|file_utils.py:1402] 2021-03-31 14:20:14,752 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzgq56q5r\r\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 4.86MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-03-31 14:20:14,861 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1409] 2021-03-31 14:20:14,862 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1402] 2021-03-31 14:20:14,932 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd1i21ypy\r\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 7.33MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-03-31 14:20:15,064 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1409] 2021-03-31 14:20:15,065 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1402] 2021-03-31 14:20:15,243 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwxm5kbhk\r\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 6.88kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-03-31 14:20:15,307 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1409] 2021-03-31 14:20:15,307 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-31 14:20:15,308 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-31 14:20:15,308 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-31 14:20:15,308 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-31 14:20:15,308 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-31 14:20:15,308 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1402] 2021-03-31 14:20:15,428 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2thkuu5h\r\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:12<00:00, 34.6MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-03-31 14:20:28,161 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|file_utils.py:1409] 2021-03-31 14:20:28,161 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|modeling_utils.py:1051] 2021-03-31 14:20:28,162 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[WARNING|modeling_utils.py:1159] 2021-03-31 14:20:31,860 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "[WARNING|modeling_utils.py:1170] 2021-03-31 14:20:31,860 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "100%|█████████████████████████████████████████| 562/562 [02:31<00:00,  3.70ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-03-31 14:23:10,413 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags.\r\n",
      "[INFO|trainer.py:988] 2021-03-31 14:23:10,626 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:989] 2021-03-31 14:23:10,627 >>   Num examples = 561465\r\n",
      "[INFO|trainer.py:990] 2021-03-31 14:23:10,627 >>   Num Epochs = 1\r\n",
      "[INFO|trainer.py:991] 2021-03-31 14:23:10,627 >>   Instantaneous batch size per device = 8\r\n",
      "[INFO|trainer.py:992] 2021-03-31 14:23:10,627 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\r\n",
      "[INFO|trainer.py:993] 2021-03-31 14:23:10,627 >>   Gradient Accumulation steps = 1\r\n",
      "[INFO|trainer.py:994] 2021-03-31 14:23:10,627 >>   Total optimization steps = 70184\r\n",
      "{'loss': 0.027, 'learning_rate': 4.9643793457198225e-05, 'epoch': 0.01}\r\n",
      "{'loss': 0.0145, 'learning_rate': 4.928758691439644e-05, 'epoch': 0.01}\r\n",
      "{'loss': 0.0095, 'learning_rate': 4.8931380371594664e-05, 'epoch': 0.02}\r\n",
      "{'loss': 0.0107, 'learning_rate': 4.8575173828792894e-05, 'epoch': 0.03}\r\n",
      "{'loss': 0.0071, 'learning_rate': 4.821896728599111e-05, 'epoch': 0.04}\r\n",
      "{'loss': 0.0064, 'learning_rate': 4.786276074318933e-05, 'epoch': 0.04}\r\n",
      "{'loss': 0.0059, 'learning_rate': 4.7506554200387556e-05, 'epoch': 0.05}\r\n",
      "{'loss': 0.0066, 'learning_rate': 4.715034765758577e-05, 'epoch': 0.06}\r\n",
      "{'loss': 0.0061, 'learning_rate': 4.6794141114783995e-05, 'epoch': 0.06}\r\n",
      "{'loss': 0.0074, 'learning_rate': 4.6437934571982224e-05, 'epoch': 0.07}\r\n",
      "{'loss': 0.0052, 'learning_rate': 4.608172802918044e-05, 'epoch': 0.08}\r\n",
      "{'loss': 0.0048, 'learning_rate': 4.572552148637866e-05, 'epoch': 0.09}\r\n",
      "{'loss': 0.0054, 'learning_rate': 4.5369314943576886e-05, 'epoch': 0.09}\r\n",
      "{'loss': 0.0065, 'learning_rate': 4.501310840077511e-05, 'epoch': 0.1}\r\n",
      "{'loss': 0.0055, 'learning_rate': 4.4656901857973325e-05, 'epoch': 0.11}\r\n",
      "{'loss': 0.0069, 'learning_rate': 4.4300695315171555e-05, 'epoch': 0.11}\r\n",
      "{'loss': 0.0051, 'learning_rate': 4.394448877236977e-05, 'epoch': 0.12}\r\n",
      "{'loss': 0.0055, 'learning_rate': 4.3588282229567994e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.0043, 'learning_rate': 4.323207568676622e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.0052, 'learning_rate': 4.287586914396444e-05, 'epoch': 0.14}\r\n",
      "{'loss': 0.0057, 'learning_rate': 4.2519662601162656e-05, 'epoch': 0.15}\r\n",
      "{'loss': 0.0041, 'learning_rate': 4.2163456058360885e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.0058, 'learning_rate': 4.180724951555911e-05, 'epoch': 0.16}\r\n",
      "{'loss': 0.0046, 'learning_rate': 4.1451042972757324e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.0028, 'learning_rate': 4.109483642995555e-05, 'epoch': 0.18}\r\n",
      "{'loss': 0.0037, 'learning_rate': 4.073862988715377e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.0033, 'learning_rate': 4.0382423344351986e-05, 'epoch': 0.19}\r\n",
      "{'loss': 0.0051, 'learning_rate': 4.0026216801550216e-05, 'epoch': 0.2}\r\n",
      "{'loss': 0.0042, 'learning_rate': 3.967001025874844e-05, 'epoch': 0.21}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.9313803715946655e-05, 'epoch': 0.21}\r\n",
      " 21%|███████▍                           | 15000/70184 [52:30<3:13:04,  4.76it/s][INFO|trainer.py:1600] 2021-03-31 15:15:41,610 >> Saving model checkpoint to ./output/checkpoint-15000\r\n",
      "[INFO|configuration_utils.py:318] 2021-03-31 15:15:41,611 >> Configuration saved in ./output/checkpoint-15000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-03-31 15:15:44,538 >> Model weights saved in ./output/checkpoint-15000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-03-31 15:15:44,544 >> tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-03-31 15:15:44,544 >> Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\r\n",
      "{'loss': 0.0038, 'learning_rate': 3.895759717314488e-05, 'epoch': 0.22}\r\n",
      "{'loss': 0.0045, 'learning_rate': 3.86013906303431e-05, 'epoch': 0.23}\r\n",
      "{'loss': 0.0037, 'learning_rate': 3.8245184087541316e-05, 'epoch': 0.24}\r\n",
      "{'loss': 0.0042, 'learning_rate': 3.7888977544739546e-05, 'epoch': 0.24}\r\n",
      "{'loss': 0.004, 'learning_rate': 3.753277100193777e-05, 'epoch': 0.25}\r\n",
      "{'loss': 0.0046, 'learning_rate': 3.7176564459135985e-05, 'epoch': 0.26}\r\n",
      "{'loss': 0.0034, 'learning_rate': 3.682035791633421e-05, 'epoch': 0.26}\r\n",
      "{'loss': 0.0031, 'learning_rate': 3.646415137353243e-05, 'epoch': 0.27}\r\n",
      "{'loss': 0.0032, 'learning_rate': 3.610794483073065e-05, 'epoch': 0.28}\r\n",
      "{'loss': 0.0033, 'learning_rate': 3.5751738287928877e-05, 'epoch': 0.28}\r\n",
      "{'loss': 0.0045, 'learning_rate': 3.53955317451271e-05, 'epoch': 0.29}\r\n",
      "{'loss': 0.0033, 'learning_rate': 3.5039325202325316e-05, 'epoch': 0.3}\r\n",
      "{'loss': 0.0063, 'learning_rate': 3.468311865952354e-05, 'epoch': 0.31}\r\n",
      "{'loss': 0.0035, 'learning_rate': 3.432691211672176e-05, 'epoch': 0.31}\r\n",
      "{'loss': 0.0028, 'learning_rate': 3.3970705573919984e-05, 'epoch': 0.32}\r\n",
      "{'loss': 0.0036, 'learning_rate': 3.361449903111821e-05, 'epoch': 0.33}\r\n",
      "{'loss': 0.0039, 'learning_rate': 3.325829248831643e-05, 'epoch': 0.33}\r\n",
      "{'loss': 0.0037, 'learning_rate': 3.2902085945514646e-05, 'epoch': 0.34}\r\n",
      "{'loss': 0.0044, 'learning_rate': 3.254587940271287e-05, 'epoch': 0.35}\r\n",
      "{'loss': 0.0035, 'learning_rate': 3.218967285991109e-05, 'epoch': 0.36}\r\n",
      "{'loss': 0.0043, 'learning_rate': 3.1833466317109315e-05, 'epoch': 0.36}\r\n",
      "{'loss': 0.003, 'learning_rate': 3.147725977430754e-05, 'epoch': 0.37}\r\n",
      "{'loss': 0.0035, 'learning_rate': 3.112105323150576e-05, 'epoch': 0.38}\r\n",
      "{'loss': 0.0027, 'learning_rate': 3.076484668870398e-05, 'epoch': 0.38}\r\n",
      "{'loss': 0.0045, 'learning_rate': 3.04086401459022e-05, 'epoch': 0.39}\r\n",
      "{'loss': 0.0033, 'learning_rate': 3.0052433603100422e-05, 'epoch': 0.4}\r\n",
      "{'loss': 0.0032, 'learning_rate': 2.969622706029864e-05, 'epoch': 0.41}\r\n",
      "{'loss': 0.0038, 'learning_rate': 2.9340020517496868e-05, 'epoch': 0.41}\r\n",
      "{'loss': 0.0038, 'learning_rate': 2.898381397469509e-05, 'epoch': 0.42}\r\n",
      "{'loss': 0.0031, 'learning_rate': 2.862760743189331e-05, 'epoch': 0.43}\r\n",
      " 43%|██████████████                   | 30000/70184 [1:45:12<2:19:04,  4.82it/s][INFO|trainer.py:1600] 2021-03-31 16:08:23,528 >> Saving model checkpoint to ./output/checkpoint-30000\r\n",
      "[INFO|configuration_utils.py:318] 2021-03-31 16:08:23,530 >> Configuration saved in ./output/checkpoint-30000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-03-31 16:08:25,067 >> Model weights saved in ./output/checkpoint-30000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-03-31 16:08:25,067 >> tokenizer config file saved in ./output/checkpoint-30000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-03-31 16:08:25,068 >> Special tokens file saved in ./output/checkpoint-30000/special_tokens_map.json\r\n",
      "{'loss': 0.0032, 'learning_rate': 2.8271400889091533e-05, 'epoch': 0.43}\r\n",
      "{'loss': 0.0045, 'learning_rate': 2.7915194346289753e-05, 'epoch': 0.44}\r\n",
      "{'loss': 0.0031, 'learning_rate': 2.7558987803487972e-05, 'epoch': 0.45}\r\n",
      "{'loss': 0.0051, 'learning_rate': 2.72027812606862e-05, 'epoch': 0.46}\r\n",
      "{'loss': 0.0035, 'learning_rate': 2.684657471788442e-05, 'epoch': 0.46}\r\n",
      "{'loss': 0.0033, 'learning_rate': 2.649036817508264e-05, 'epoch': 0.47}\r\n",
      "{'loss': 0.0031, 'learning_rate': 2.6134161632280864e-05, 'epoch': 0.48}\r\n",
      "{'loss': 0.0041, 'learning_rate': 2.5777955089479083e-05, 'epoch': 0.48}\r\n",
      "{'loss': 0.0033, 'learning_rate': 2.5421748546677303e-05, 'epoch': 0.49}\r\n",
      "{'loss': 0.0028, 'learning_rate': 2.5065542003875532e-05, 'epoch': 0.5}\r\n",
      "{'loss': 0.0063, 'learning_rate': 2.470933546107375e-05, 'epoch': 0.51}\r\n",
      "{'loss': 0.0044, 'learning_rate': 2.435312891827197e-05, 'epoch': 0.51}\r\n",
      "{'loss': 0.0036, 'learning_rate': 2.3996922375470194e-05, 'epoch': 0.52}\r\n",
      "{'loss': 0.0029, 'learning_rate': 2.3640715832668417e-05, 'epoch': 0.53}\r\n",
      "{'loss': 0.0046, 'learning_rate': 2.3284509289866636e-05, 'epoch': 0.53}\r\n",
      "{'loss': 0.0045, 'learning_rate': 2.292830274706486e-05, 'epoch': 0.54}\r\n",
      "{'loss': 0.0045, 'learning_rate': 2.2572096204263082e-05, 'epoch': 0.55}\r\n",
      "{'loss': 0.0036, 'learning_rate': 2.22158896614613e-05, 'epoch': 0.56}\r\n",
      "{'loss': 0.0028, 'learning_rate': 2.1859683118659524e-05, 'epoch': 0.56}\r\n",
      "{'loss': 0.0037, 'learning_rate': 2.1503476575857744e-05, 'epoch': 0.57}\r\n",
      "{'loss': 0.0025, 'learning_rate': 2.114727003305597e-05, 'epoch': 0.58}\r\n",
      "{'loss': 0.0034, 'learning_rate': 2.079106349025419e-05, 'epoch': 0.58}\r\n",
      "{'loss': 0.0023, 'learning_rate': 2.043485694745241e-05, 'epoch': 0.59}\r\n",
      "{'loss': 0.0035, 'learning_rate': 2.0078650404650635e-05, 'epoch': 0.6}\r\n",
      "{'loss': 0.0026, 'learning_rate': 1.9722443861848855e-05, 'epoch': 0.61}\r\n",
      "{'loss': 0.0028, 'learning_rate': 1.9366237319047074e-05, 'epoch': 0.61}\r\n",
      "{'loss': 0.0022, 'learning_rate': 1.90100307762453e-05, 'epoch': 0.62}\r\n",
      "{'loss': 0.0032, 'learning_rate': 1.865382423344352e-05, 'epoch': 0.63}\r\n",
      "{'loss': 0.0028, 'learning_rate': 1.829761769064174e-05, 'epoch': 0.63}\r\n",
      "{'loss': 0.0021, 'learning_rate': 1.7941411147839966e-05, 'epoch': 0.64}\r\n",
      " 64%|█████████████████████▏           | 45000/70184 [2:37:25<1:22:53,  5.06it/s][INFO|trainer.py:1600] 2021-03-31 17:00:36,396 >> Saving model checkpoint to ./output/checkpoint-45000\r\n",
      "[INFO|configuration_utils.py:318] 2021-03-31 17:00:36,397 >> Configuration saved in ./output/checkpoint-45000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-03-31 17:00:37,838 >> Model weights saved in ./output/checkpoint-45000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-03-31 17:00:37,839 >> tokenizer config file saved in ./output/checkpoint-45000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-03-31 17:00:37,839 >> Special tokens file saved in ./output/checkpoint-45000/special_tokens_map.json\r\n",
      "{'loss': 0.0028, 'learning_rate': 1.7585204605038185e-05, 'epoch': 0.65}\r\n",
      "{'loss': 0.0024, 'learning_rate': 1.7228998062236408e-05, 'epoch': 0.66}\r\n",
      "{'loss': 0.0021, 'learning_rate': 1.687279151943463e-05, 'epoch': 0.66}\r\n",
      "{'loss': 0.0016, 'learning_rate': 1.651658497663285e-05, 'epoch': 0.67}\r\n",
      "{'loss': 0.0029, 'learning_rate': 1.6160378433831073e-05, 'epoch': 0.68}\r\n",
      "{'loss': 0.002, 'learning_rate': 1.5804171891029296e-05, 'epoch': 0.68}\r\n",
      "{'loss': 0.0019, 'learning_rate': 1.5447965348227516e-05, 'epoch': 0.69}\r\n",
      "{'loss': 0.0029, 'learning_rate': 1.5091758805425737e-05, 'epoch': 0.7}\r\n",
      "{'loss': 0.0025, 'learning_rate': 1.4735552262623962e-05, 'epoch': 0.71}\r\n",
      "{'loss': 0.0036, 'learning_rate': 1.4379345719822183e-05, 'epoch': 0.71}\r\n",
      "{'loss': 0.0031, 'learning_rate': 1.4023139177020402e-05, 'epoch': 0.72}\r\n",
      "{'loss': 0.0017, 'learning_rate': 1.3666932634218627e-05, 'epoch': 0.73}\r\n",
      "{'loss': 0.002, 'learning_rate': 1.3310726091416848e-05, 'epoch': 0.73}\r\n",
      "{'loss': 0.0013, 'learning_rate': 1.2954519548615069e-05, 'epoch': 0.74}\r\n",
      "{'loss': 0.0022, 'learning_rate': 1.2598313005813292e-05, 'epoch': 0.75}\r\n",
      "{'loss': 0.0031, 'learning_rate': 1.2242106463011513e-05, 'epoch': 0.76}\r\n",
      "{'loss': 0.0011, 'learning_rate': 1.1885899920209734e-05, 'epoch': 0.76}\r\n",
      "{'loss': 0.0024, 'learning_rate': 1.1529693377407957e-05, 'epoch': 0.77}\r\n",
      "{'loss': 0.002, 'learning_rate': 1.1173486834606178e-05, 'epoch': 0.78}\r\n",
      "{'loss': 0.0016, 'learning_rate': 1.0817280291804401e-05, 'epoch': 0.78}\r\n",
      "{'loss': 0.0021, 'learning_rate': 1.0461073749002622e-05, 'epoch': 0.79}\r\n",
      "{'loss': 0.0022, 'learning_rate': 1.0104867206200844e-05, 'epoch': 0.8}\r\n",
      "{'loss': 0.0022, 'learning_rate': 9.748660663399067e-06, 'epoch': 0.81}\r\n",
      "{'loss': 0.0016, 'learning_rate': 9.392454120597288e-06, 'epoch': 0.81}\r\n",
      "{'loss': 0.0022, 'learning_rate': 9.036247577795509e-06, 'epoch': 0.82}\r\n",
      "{'loss': 0.0022, 'learning_rate': 8.680041034993732e-06, 'epoch': 0.83}\r\n",
      "{'loss': 0.002, 'learning_rate': 8.323834492191953e-06, 'epoch': 0.83}\r\n",
      "{'loss': 0.0017, 'learning_rate': 7.967627949390174e-06, 'epoch': 0.84}\r\n",
      "{'loss': 0.0024, 'learning_rate': 7.611421406588397e-06, 'epoch': 0.85}\r\n",
      "{'loss': 0.0014, 'learning_rate': 7.255214863786617e-06, 'epoch': 0.85}\r\n",
      " 85%|█████████████████████████████▉     | 60000/70184 [3:29:53<34:54,  4.86it/s][INFO|trainer.py:1600] 2021-03-31 17:53:03,994 >> Saving model checkpoint to ./output/checkpoint-60000\r\n",
      "[INFO|configuration_utils.py:318] 2021-03-31 17:53:03,995 >> Configuration saved in ./output/checkpoint-60000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-03-31 17:53:05,434 >> Model weights saved in ./output/checkpoint-60000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-03-31 17:53:05,435 >> tokenizer config file saved in ./output/checkpoint-60000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-03-31 17:53:05,435 >> Special tokens file saved in ./output/checkpoint-60000/special_tokens_map.json\r\n",
      "{'loss': 0.0019, 'learning_rate': 6.89900832098484e-06, 'epoch': 0.86}\r\n",
      "{'loss': 0.0017, 'learning_rate': 6.542801778183062e-06, 'epoch': 0.87}\r\n",
      "{'loss': 0.0019, 'learning_rate': 6.186595235381284e-06, 'epoch': 0.88}\r\n",
      "{'loss': 0.0022, 'learning_rate': 5.830388692579505e-06, 'epoch': 0.88}\r\n",
      "{'loss': 0.0022, 'learning_rate': 5.474182149777727e-06, 'epoch': 0.89}\r\n",
      "{'loss': 0.002, 'learning_rate': 5.1179756069759494e-06, 'epoch': 0.9}\r\n",
      "{'loss': 0.0016, 'learning_rate': 4.761769064174171e-06, 'epoch': 0.9}\r\n",
      "{'loss': 0.002, 'learning_rate': 4.405562521372393e-06, 'epoch': 0.91}\r\n",
      "{'loss': 0.0019, 'learning_rate': 4.049355978570615e-06, 'epoch': 0.92}\r\n",
      "{'loss': 0.0018, 'learning_rate': 3.6931494357688363e-06, 'epoch': 0.93}\r\n",
      "{'loss': 0.0023, 'learning_rate': 3.3369428929670583e-06, 'epoch': 0.93}\r\n",
      "{'loss': 0.0024, 'learning_rate': 2.98073635016528e-06, 'epoch': 0.94}\r\n",
      "{'loss': 0.0018, 'learning_rate': 2.6245298073635015e-06, 'epoch': 0.95}\r\n",
      "{'loss': 0.0019, 'learning_rate': 2.2683232645617235e-06, 'epoch': 0.95}\r\n",
      "{'loss': 0.0016, 'learning_rate': 1.9121167217599455e-06, 'epoch': 0.96}\r\n",
      "{'loss': 0.001, 'learning_rate': 1.5559101789581671e-06, 'epoch': 0.97}\r\n",
      "{'loss': 0.001, 'learning_rate': 1.199703636156389e-06, 'epoch': 0.98}\r\n",
      "{'loss': 0.001, 'learning_rate': 8.434970933546108e-07, 'epoch': 0.98}\r\n",
      "{'loss': 0.0027, 'learning_rate': 4.872905505528326e-07, 'epoch': 0.99}\r\n",
      "{'loss': 0.0015, 'learning_rate': 1.3108400775105438e-07, 'epoch': 1.0}\r\n",
      "100%|███████████████████████████████████| 70184/70184 [4:05:37<00:00,  4.93it/s][INFO|trainer.py:1171] 2021-03-31 18:28:47,822 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 14737.1947, 'train_samples_per_second': 4.762, 'epoch': 1.0}\r\n",
      "100%|███████████████████████████████████| 70184/70184 [4:05:37<00:00,  4.76it/s]\r\n",
      "[INFO|trainer.py:1600] 2021-03-31 18:28:48,316 >> Saving model checkpoint to ./output\r\n",
      "[INFO|configuration_utils.py:318] 2021-03-31 18:28:48,317 >> Configuration saved in ./output/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-03-31 18:28:49,654 >> Model weights saved in ./output/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-03-31 18:28:49,654 >> tokenizer config file saved in ./output/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-03-31 18:28:49,655 >> Special tokens file saved in ./output/special_tokens_map.json\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-03-31 18:28:49,685 >> ***** train metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   epoch                      =        1.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   init_mem_cpu_alloc_delta   =     1572MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   init_mem_cpu_peaked_delta  =      300MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   init_mem_gpu_alloc_delta   =      411MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   init_mem_gpu_peaked_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_mem_cpu_alloc_delta  =      381MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_mem_cpu_peaked_delta =      267MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_mem_gpu_alloc_delta  =     1234MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_mem_gpu_peaked_delta =     6213MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_runtime              = 4:05:37.19\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_samples              =     561465\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-03-31 18:28:49,685 >>   train_samples_per_second   =      4.762\r\n"
     ]
    }
   ],
   "source": [
    "!python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "--model_name_or_path 'bert-base-cased' \\\n",
    "--train_file './train_ner.json' \\\n",
    "--validation_file './train_ner.json' \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--save_steps 15000 \\\n",
    "--output_dir './output' \\\n",
    "--report_to 'none' \\\n",
    "--seed 123 \\\n",
    "--do_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 23.947845,
     "end_time": "2021-03-31T18:29:39.777446",
     "exception": false,
     "start_time": "2021-03-31T18:29:15.829601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After the tuning finishes, we should find our model in './output'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15378.251339,
   "end_time": "2021-03-31T18:30:08.970193",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-31T14:13:50.718854",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
